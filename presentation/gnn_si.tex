\documentclass{beamer}

\usepackage[utf8]{inputenc}
\usepackage{color}
\usepackage{amsfonts}	% Real

\usepackage{booktabs} % eleganckie tabelki


\usetheme{boxes}      % Wybór tematu wyglądu, gdy chcemy inny
%\usecolortheme{rose}   % Wybór tematu kolorystycznego, j.w.

%Konfiguracja dla pakietu hyperref:
\hypersetup{
  unicode=true,           % włączenie wyświetlania pliterek w zakładkach
%  pdfpagemode=FullScreen, % włączenie trybu pełnoekranowanego
  pdfsubject=Graph Neural Networks,      % temat prezentacji
  pdfkeywords={gnn, graph neural network, graph, classification} % slowa kluczowe
}

%% Dane do strony tytułowej
\author{Aleksy Barcz, Stanisław Jankowski}
\title{Graph Neural Networks for crystal structures}
\date{\today}
\institute{Warsaw University of Technology}

\setbeamercovered{transparent}

\begin{document}
\frame{\titlepage}

\begin{frame}
\frametitle{Graphs - are FNNs adequate?}
\begin{columns}
	\begin{column}{0.66\textwidth}
		\begin{itemize}
			\item input = vector
			\item constant input size
			\item information about structure is lost
			\item encodings are handcrafted
		\end{itemize}
	\end{column}
	\begin{column}{0.34\textwidth}
		\includegraphics[scale=0.4]{img/graph}
	\end{column}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Graph-oriented neural networks}
\begin{columns}
	\begin{column}{0.66\textwidth}
		\begin{itemize}
			\item process information about graph structure directly
			\item learn a meaningful representation
			\item capable of classification and regression
			\item based on feed-forward neural networks
		\end{itemize}
	\end{column}
	\begin{column}{0.34\textwidth}
		\includegraphics[scale=0.4]{img/graph}
	\end{column}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Applications of graph-oriented neural networks}
\begin{itemize}
	\item drug design
	\item prediction of fuel compounds properties
	\item object detection in 2D images
	\item document mining
	\item web page ranking
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Graph Neural Networks}
\begin{center}
	\includegraphics[scale=0.4]{img/encodinginc}
\end{center}
\begin{itemize}
	\item building node representation: $x_n = f(...)$
	\item node classification: $o_n = g(x_n)$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Application to crystal structures}
\begin{itemize}
	\item data model should include: \begin{itemize}
		\item spatial location of atoms and vacancies
		\item bonds
		\item atom, vacancy, bond properties
		\item crystal cell properties
	\end{itemize}
	\item highly cyclic data
	\item first approximation: Bravais lattices
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Data structure}
\begin{columns}
	\begin{column}{0.66\textwidth}
		\begin{itemize}
			\item crystals represented as Bravais lattices
			\item graph = unit cell
			\item node = lattice point
			\item edge = lattice edge (2 directed edges per lattice edge)
			\item output evaluated at every node
		\end{itemize}
	\end{column}
	\begin{column}{0.34\textwidth}
		\includegraphics[scale=0.45]{img/tetragonal-I}
	\end{column}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Parametrisation}
\begin{itemize}
	\item edge label - 3D vector coordinates, bond properties
	\item node label - physicochemical atom properties
	\item output - if set to same value for each node, can represent a whole-crystal property
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Experiment: distinguishing crystals with substitutions}
\begin{columns}
	\begin{column}{0.34\textwidth}
		\includegraphics[scale=0.45]{img/tetragonal-I}
	\end{column}
	\begin{column}{0.66\textwidth}
		\begin{itemize}
			\item tetragonal body-centered cells
			\item distinguish crystals with single substitution (different node label)
			\item distinguish crystals with substitution placed at different lattice point
		\end{itemize}
	\end{column}
\end{columns}

\end{frame}

\begin{frame}
\frametitle{Experiment: dataset generation}
\begin{itemize}
	\item random scaling of edges maintaining the tetragonal constraint
	\item random error added to node labels\\(atom description perturbation)
	\item random error added to edge labels\\(atom distances perturbation)
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Experiment: training and evaluation}
\begin{itemize}
	\item training set: 50 graphs from each class (total: 100)
	\item test set: 150 graphs from each class (total: 300)
	\item classifier with smallest RMSE on training set chosen
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Experiment: results}

\begin{table}[h!]
\begin{center}
\caption{Substitution existence}
\begin{tabular}{llll}
\hline\noalign{\smallskip}
 & accuracy & precision & recall\\
\noalign{\smallskip}
\hline
\noalign{\smallskip}
training set & 96.00\% &  96.00\% & 96.00\% \\
test set & 94.92\% & 95.29\% & 94.51\% \\
\hline
\label{tab:subst}
\end{tabular}
\end{center}
\end{table}

\begin{table}[h!]
\begin{center}
\caption{Substitution location difference}
\begin{tabular}{llll}
\hline\noalign{\smallskip}
 & accuracy & precision & recall\\
\noalign{\smallskip}
\hline
\noalign{\smallskip}
training set & 96.33\% &  94.26\% & 98.66\% \\
test set & 95.07\% & 92.05\% & 98.66\% \\
\hline
\label{tab:substloc}
\end{tabular}
\end{center}
\end{table}

\end{frame}

\begin{frame}
\frametitle{Application to Si defects - future challenges}
\begin{columns}
	\begin{column}{0.5\textwidth}
		\includegraphics[scale=0.15]{img/Silicon-unit-cell-3D-balls.png}
	\end{column}
	\begin{column}{0.5\textwidth}
		\begin{itemize}
			\item approximating activation energy
			\item different defect classes
			\item vacancies
			\item different types of vacancies
		\end{itemize}
	\end{column}
\end{columns}
\end{frame}

\begin{frame}
\begin{center}
\Huge{\emph{Thank you}}
\end{center}
\end{frame}

\begin{frame}
\frametitle{GNN - learning algorithm}
\begin{enumerate}
	\item initialize randomly $h_w$ and $g_w$ weights
	\item until stop criterion is satisfied:
	\begin{itemize}
		\item random initialization of representation $X$
		\item FORWARD : calculate $X = F_w(X)$ until fixed point is reached
		\item BACKWARD : calculate $G_w(X)$ and backpropagate the error
		\item update $f_w$ and $g_w$ weights
	\end{itemize}
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{Forward - building representation}
\begin{columns}
	\begin{column}{0.66\textwidth}
		\includegraphics[scale=0.4]{img/forward2}
	\end{column}
	\begin{column}{0.34\textwidth}
		\begin{itemize}
			\item unfolding
		\end{itemize}
	\end{column}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Backward - backpropagation}
\begin{columns}
	\begin{column}{0.66\textwidth}
		\includegraphics[scale=0.4]{img/backward2}
	\end{column}
	\begin{column}{0.34\textwidth}
		\begin{itemize}
			\item BPTT
			\item Almeida-Pineda
		\end{itemize}
	\end{column}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{How do we know $F_w$ will reach fixed point?}
\begin{itemize}
	\item contraction map (Banach theorem)
	\item $||F_w(X_1) - F_w(X_2)|| \leq ||X_1 - X_2||$
	\item unique fixed point
	\item fixed point reached from every starting point
	\item very few iterations needed
	\item penalty imposed on $F_w$ weights when the contraction is lost
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{How strong should the penalty be?}
\begin{center}
	\includegraphics[scale=0.065]{img/rmse1_clipped}
\end{center}
\begin{itemize}
	\item $contractionConstant$
	\item boundary value depends on dataset
\end{itemize}
\end{frame}

\end{document}

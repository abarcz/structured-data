
\chapter{Experiments~\label{ch:experiments}}
Experiments were conducted to check if the implemented GNN is able to cope with the tasks presented in the original article~\cite{scarselli2009graph}. For all the experiments the state size was set to 5, the number of hidden neurons in both the $h_{\bm{w}}$ and $g_{\bm{w}}$ networks was set to 5. After a couple successful trivial experiments, consisting of memorizing a single graph, the proper experiments were conducted. The task chosen for experiments was the \emph{subgraph matching} task. It was chosen, because:
\begin{enumerate}
	\item a similar experiment was conducted by Scarselli et al.~\cite{scarselli2009graph}
	\item the dataset is easy to generate, yet the problem is not trivial
	\item to yield good results, the structure of the graph have to be exploited.
\end{enumerate}

\section{Subgraph matching - data}
The datasets for the subgraph matching task were generated as follows. For a given number of graph nodes, graphs were generated by selecting node labels from $[0..10]$ and then connecting each node pair in a graph with an edge probability $\delta$. Then, random edges were added to the graph until the graph became connected. Then, a smaller connected subgraph was inserted to every graph in the dataset. Then, a simple algorithm was used to locate all the copies of subgraph in every graph in the dataset. Thus, every graph in the dataset contained at least on copy of the subgraph. Afterwards, a small Gaussian noise with zero mean and standard deviation of $0.25$ was added to node labels. All the graph edges were undirected and thus were transformed to pairs of directed edges prior to processing. No edge labels were used.

Two datasets were generated. One with graph size (number of nodes) equal to 6, the subgraph size equal to 3 and $\delta = 0.8$ (100 graphs, called later the \emph{6-3 dataset}). The second one with graph size equal to 14, subgraph size equal to 7 and $\delta = 0.2$ (100 graphs, called later the \emph{14-7 dataset}). A larger $\delta$ was used for the first dataset, as graphs generated with $\delta = 0.2$ were mostly sequences. The first dataset was used to analyze the process of training, while the second one was used for comparison of GNN with a standard FNN classifier.

\section{Impact of initial weight values on learning}
To test the impact of initial weight values on the process of a GNN training, 9 different sets of weights were tested. For all tested networks, the contraction constant ($\mu$ from Eq.~\ref{eq:gnn_pw}) was set to $0.9$. The training was performed on 10 graphs belonging to the 6-3 dataset. Each GNN network was trained for 50 iterations. As the default error measure used for GNN training is the Mean Square Error, a connected performance measure - RMSE was used for evaluation. Results are presented in Fig.~\ref{fig:gnn_multiple}. Out of 9 networks, only 4 performed well: gnn2, gnn3, gnn5 and gnn7. The gnn5 network yielded the smallest RMSE at the end of training and also presents a remarkably monotonous RMSE slope compared with gnn7. All the other networks didn't improve significantly on the RMSE value, which may suggest that multiple initial values of weights should be tried for a given problem to build an efficient classifier.

\begin{figure}[h!]
\begin{center}
	\includegraphics[scale=0.09]{img/rmse_gnn1-9}
	\caption{RMSE for 9 different initial weight sets. $\mu = 0.9$}
	\label{fig:gnn_multiple}
\end{center}
\end{figure}

\newpage
\section{Impact of contraction constant on learning}
During the initial experiments, interesting results were obtained for different values of the contraction constant ($\mu$ from Eq.~\ref{eq:gnn_pw}). It seems that for a given learning task exists a minimum value of $\mu$ below which no learning occurs. Some experiments were conducted for the 6-3 dataset using the best networks from Fig.~\ref{fig:gnn_multiple}: the gnn5 and gnn7 network (initial weights were used). The results for gnn5 are presented in Fig.~\ref{fig:gnn5} and the results for gnn7 are presented in Fig.~\ref{fig:gnn7}. For both networks three different value of $\mu$ were tested: 1.2, 0.9 and 0.6. In both cases it can be observed that no training occurs for $\mu = 0.6$. For these experiments 20 graphs from the 6-3 dataset were used.

\begin{figure}[h!]
\begin{center}
	\includegraphics[scale=0.09]{img/rmse_clipped}
	\caption{RMSE for gnn5 with $\mu \in [1.2, 0.9, 0.6]$}
	\label{fig:gnn5}
\end{center}
\end{figure}

\begin{figure}[h!]
\begin{center}
	\includegraphics[scale=0.09]{img/rmse1_clipped}
	\caption{RMSE for gnn7 with $\mu \in [1.2, 0.9, 0.6]$}
	\label{fig:gnn7}
\end{center}
\end{figure}

A closer look on the process of learning may shed some light on the reasons behind the lack of learning. In Fig.~\ref{fig:gnn7_09} the process of learning of gnn7 with $\mu = 0.9$ are presented. In Fig.~\ref{fig:gnn7_06} the same network gnn7 was trained with $\mu = 0.6$. The different values shown are: \emph{nForward} - number of \emph{Forward} state building iterations, \emph{nBackward} - number of \emph{Backward} error accumulation iterations, \emph{penalty} - was any weight penalized, \emph{de/dw influence} - percent of combined weight updates that had the sign same as $\frac{\partial e}{\partial w}$ (before passing to RPROP algorithm), \emph{dp/dw influence} - percent of combined weight updates that had the sign same as $\frac{\partial p}{\partial w}$. Some interesting features of the GNN model learning schema can be observed. In the case of $\mu = 0.9$ the number of \emph{Forward} steps reached the maximum a couple of times, which presumably means that at that time the $F_{\bm{w}}$ ceased being a contraction map. The penalty was imposed mostly for short periods of time and only at one moment caused the $\frac{\partial e}{\partial w}$ influence to drop below 50\%. This strategy yielded good results - the penalty imposed reduced the number of \emph{Forward} steps and the RMSE was successfully reduced.

\begin{figure}[h!]
\begin{center}
	\includegraphics[scale=0.09]{img/gnn1_2}
	\caption{gnn7 performance with $\mu = 0.9$}
	\label{fig:gnn7_09}
\end{center}
\end{figure}

A different situation is shown for $\mu = 0.6$. Because of a low $\mu$ value, the penalty was imposed hastily and was larger than in the previous case (the impact of the $\mu$ value was shown in Eq.~\ref{eq:gnn_pw_L}). It was imposed even when the number of \emph{Forward} steps was below the maximum, that is when $F_{\bm{w}}$ was still a contraction map. Large values of the penalty caused a huge decrease of the $\frac{\partial e}{\partial w}$ term influence, which made any learning impossible.

\begin{figure}[h!]
\begin{center}
	\includegraphics[scale=0.09]{img/gnn1_3}
	\caption{gnn7 performance with $\mu = 0.6$}
	\label{fig:gnn7_06}
\end{center}
\end{figure}

\newpage
Another interesting case is presented in Fig.~\ref{fig:gnn5_09}. The learning process of gnn5 on 10 graphs with $\mu = 0.9$ is presented. It can be observed that even as the number of \emph{Forward} steps reached in peaks the maximum value, the $F_{\bm{w}}$ function remained a contraction map. A large enough $\mu$ prevented the penalty from being imposed, which enabled the GNN model to train both computation units without any disturbance. The result is a monotonously decreasing RMSE slope, which could be previously observed in Fig.~\ref{fig:gnn_multiple}. It can be concluded, that the most important aspect of building a GNN model is to provide an efficient way to make $F_{\bm{w}}$ a contraction map as fast as possible, so as to provide as much time as possible for undisturbed learning.

\newpage
\begin{figure}[h!]
\begin{center}
	\includegraphics[scale=0.09]{img/gnn5}
	\caption{gnn5 performance with $\mu = 0.9$}
	\label{fig:gnn5_09}
\end{center}
\end{figure}

\newpage
\section{Crossvalidation results}
To present the performance of the implemented GNN model compared to the performance of a standard FNN, the following subgraph matching experiment was conducted. 5-fold crossvalidation was performed on all 100 graphs from the 14-7 dataset. A random GNN was used with contraction constant $\mu = 0.9$ and it was trained from scratch for 50 iterations during each fold. To provide good FNN results, 10 three-layer FNNs with 5 hidden $tanh$ neurons were evaluated and the one with best mean accuracy was selected. The results are presented in Table~\ref{tab:crossmean} and~\ref{tab:crossstd}. It can be seen that the GNN classifier outperformed the FNN by as much as 20\%. This is due to the fact, that the FNN classifier could make predictions only by analyzing node labels, while the GNN classifier exploited correctly the graph topology.

These results can be better understood by analyzing the classified dataset. The 100 processed graphs consisted in total of 1400 nodes. Amongst these nodes, 1031 had node labels matching the subgraph node labels. Amongst these 1031 nodes only 702 actually belonged to the subgraph. Thus, 329 nodes, 23.5\% of all the nodes would probably be classified as false positives by a classifier taking into consideration only node labels. This hypothesis corresponds quite well with the results presented.

\begin{table}[h!]
	\begin{center}
	\begin{tabular}{llll}
	\toprule
	& accuracy & precision & recall \\
	\midrule
	FNN - tr &	71\% &  65\% & 93\% \\
	FNN - tst &	71\% &  64\% &  93\% \\
	GNN - tr &	91\% &  87\%&  97\% \\
	GNN - tst &	91\% &  86\% &  97\% \\
	\bottomrule
	\end{tabular}
	\caption{Mean values on training and test sets}
	\label{tab:crossmean}
	\end{center}
\end{table}

\begin{table}[h!]
	\begin{center}
	\begin{tabular}{llll}
	\toprule
	& accuracy & precision & recall \\
	\midrule
	FNN - tr &	0.34\% &  0.67\% & 1.86\% \\
	FNN - tst &	2.45\% &  1.73\% &  2.93\% \\
	GNN - tr &	1.62\% &  1.71\% &  2.07\% \\
	GNN - tst &	3.06\% &  3.70\% &  1.39\% \\
	\bottomrule
	\end{tabular}
	\caption{Standard deviations on training and test sets}
	\label{tab:crossstd}
	\end{center}
\end{table}

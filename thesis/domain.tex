
\chapter{Domains of application}
This chapter presents domains where data is organized into a structured form, that is form of sequences or graphs. The necessity of processing differently such kinds of data arose from the structure of the data itself. To present this difference we must first summarize what is the task of classification and regression in the most common sense in data processing domain. A common statistical classifier takes as input \emph{samples} from a given \emph{dataset}, representing real world objects, and associates each sample with a category. The samples are fixed-length vectors of numeric values. Each position of such a vector represents a feature of the sample, which is quantified by a real or integer value. The mapping from features to positions in the vector is fixed and must hold for each sample in the dataset. The category is represented by a non-empty fixed-length vector of integer values, where once again the position of each value is meaningful (we can say it's a \emph{positional} representation). For the regression task, instead of a category, a vector of real (or integer) values is associated with each sample. The domain of common statistical classifiers is well developed and consists of such classifiers as neural networks, support vector machines, na√Øve Bayes classifiers, random forests and others.

In the case of graph-structured data, the nature of the data is different. Each \emph{sample} is represented by a graph. A \emph{dataset} may consist of a single or several graphs. Each graph consists of nodes, connected with edges. Each node can be described by its \emph{label}, a fixed-length vector of reals. Each edge can also be labelled, with a fixed-length vector of reals of different size. Edges can be directed or undirected. An example of a simple graph was presented in Fig.~\ref{fig:simple_tree}.

\begin{figure}
\begin{center}
	\includegraphics[scale=0.4]{img/tree}
	\caption{A simple binary tree}
	\label{fig:simple_tree}
\end{center}
\end{figure}

 If we wanted to use such a graph as input for a common classifier, we would have to transform the structured representation into a plain vector. We could accomplish it in several ways, one of the most obvious would be to perform a depth-first search of the graph and list the node labels in the order they were examined. Such a search would result in the representation $[A,B,D,E,C]$. We can already see in that simple example that the explicit information about node adjacency was lost. Instead, the information is provided implicitly, according to a coding which must be known to properly interpret such a vector representation. That means that a model learning to classify such graphs would have to learn such a relationship, instead of using it from the beginning to learn other, unknown relationships affecting the samples category. The resulting learning task becomes even harder if such sequential representations contain long-distance relationships. As different forms of encoding structured data to vectors exist, they all share that flaw.

Moreover, the inadequacy of simple vector representation becomes even more apparent when the graph structure becomes more complicated. First of all, if we include edge labels in the vector representation we actually mix data belonging to two different entities - nodes and edges, without instructing the classifier which part of the data corresponds to one or another. The same applies to directed and undirected edges. If two edges in a graph are connected by a directed edge, presumably one of the nodes in the relation has got a larger impact on the other than vice versa. On the contrary, an undirected or bidirectional edge edge implies an equal impact of both nodes on each other. What if a graph contains both types of edges? Secondly, let's consider the case of cyclic dependencies. Even if a meaningful representation of the graph is built ignoring cycles, e.g. by constructing a minimum spanning tree, some explicit information about connections is lost. An example of such data would be chemical compounds containing groups of atoms forming cyclic bonds. Another problem lies in the positional nature of vectorial data. If we build a representation by simply storing node labels one after another, this representation becomes vulnerable to missing nodes and also to any reordering of children of a node. At the same time, the ordering of the children of a node may be irrelevant in the dataset considered and additional effort must be made to assure a consistent ordering of potentially difficult to order data.

It can be seen that in order to properly process structured data, a different approach must be used. The data should be processed in a way that exploits properly the information contained in its structure - by means of building a sufficient representation or by processing the structured data directly. Domains, where graph-oriented processing is already used include chemistry (QSAR, computer-aided drug design), pattern recognition and natural language processing.







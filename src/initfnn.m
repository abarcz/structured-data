
function net = initfnn(nInputLines, nHiddenNeurons, nOutputNeurons)
% Initialize weights of a three-layer FNN with logistic activation function.
% Each column of resulting matrices corresponds to a single neuron weights.
% Each neuron gets an extra weight for bias.
%
% usage: net = initfnn(nInputLines, nHiddenNeurons, nOutputNeurons)
%
% nInputLines : number of input lines, script will add +1 for bias automatically

	weights1 = initializeweights(nInputLines, nHiddenNeurons);
	bias1 = initializeweights(1, nHiddenNeurons);
	weights2 = initializeweights(nHiddenNeurons, nOutputNeurons);
	bias2 = initializeweights(1, nOutputNeurons);

	net = struct(...
		'weights1', weights1, ...
		'bias1', bias1, ...
		'weights2', weights2, ...
		'bias2', bias2, ...
		'nInputLines', nInputLines, ...
		'nHiddenNeurons', nHiddenNeurons, ...
		'nOutputNeurons', nOutputNeurons, ...
		'activation1', @(x) tanh(x), ...
		'activationderivative1', @(x) repmat(1, size(x)) - (tanh(x) .^ 2), ...
		'activation2ndderivative1', @(x) 2 .* (tanh(x) .^ 3 - tanh(x)), ...
		'activation2', @(x) tanh(x), ...
		'activationderivative2', @(x) repmat(1, size(x)) - (tanh(x) .^ 2), ...
		'activation2ndderivative2', @(x) 2 .* (tanh(x) .^ 3 - tanh(x)), ...
		'refuseVal', 0);
end

function weights = initializeweights(nInputLines, nNeurons)
	% rand returns values uniformly distributed on (0, 1)
	weights = (rand(nNeurons, nInputLines) - 0.5) .* (2 ./ nInputLines);
end
